---
title: "Homework for Statistical Rethinking"
author: "Kristoffer Klevjer"
date: 'Current / knitted date: `r Sys.Date()`'
output:
  html_document: default
subtitle: UiT, Spring 2020
---
***
### Precursers
1) Install (R)Stan. Go to [mc-stan.org](mc-stan.org), or [directly here](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started) for RStan, follow all instructions including "Loading the package". PS: I had a lot of problems the first time, as I didn't have R v3.4.0 or later and/or RStudio v1.2.x or later.
2) Install the following packages: install.packages(c("coda", "mvtnorm", "devtools", "loo"))
3) Load devtools: library(devtools)
4) Install the rethinking package: devtools::install_github("rmcelreath/rethinking", ref="Experimental")
5) Pray that you didn't get any breaking-worthy errors, I got an error in the rethinking package related to serialized objects, however as this package is solely for learning, and not using Rstan, I'll take my chances.


***
## Week One
Overall: We are tossing a small globe around the room, and record whether our right index finger lands on Water or Land when catching it. Assuming the globe is a fair representation of Earth, we can use the data we obtain to estimate the proportion of water to land on the Earth's surface.


***
### Task 1
Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

### Task 1 - My solution
In this task, we assume that we have no prior knowledge about the Earth's proportion of water to land, i.e., we have a flat prior, with all values being equally likely.

In this task, we can use grid approximation (fine grid instead of integrals, often computationally intensive with increased amounts of data/complexity).

```{r, echo=TRUE}
p_grid1 <- seq(from = 0, to = 1, length = 1000) #grid of 1000
prob_p1 <- rep(1, 1000) #flat prior
prob_data1 <- dbinom(8, size = 15, prob = p_grid1)
posterior1 <- prob_data1 * prob_p1 # taking the prior into account, not needed in this example
posteriornorm1 <- posterior1 / sum(posterior1) #normalizing
set.seed(100)
samples1 <- sample(p_grid1, prob = posteriornorm1, size = 1000, replace = TRUE)
mean(samples1) #point estimate of the true proportion of water
quantile(samples1, probs = c(0.05, 0.95)) #95% CI
```

In short, given the prior and the observed data, our best estimate for the true proportion of water to land is ```r mean(samples1)``` (95%CI: ```r quantile(samples1, probs=(0.05))``` to ```r quantile(samples1, probs=(0.95))```).

***
### Task 2
Start over in **1**, but now use a prior that is zero below _p_ = 0.5. This corresponds to prior information that a majority of the Earth's surface is water. What difference does the better prior make? If it helps, compare posterior distributions (using both priors) to the true value _p_ = 0.7.

### Task 2 - My solution
We can use the same code as above, only we adjust the prior probability.
```{r, echo=TRUE}
p_grid2 <- seq(from = 0, to = 1, length = 1000) #grid of 1000
prob_p2 <- c(rep(0, 500), rep(1, 500)) #new prior
prob_data2 <- dbinom(8, size = 15, prob = p_grid2)
posterior2 <- prob_data2 * prob_p2 # taking the prior into account, not needed in this example
posteriornorm2 <- posterior2 / sum(posterior2) #normalizing
set.seed(100)
samples2 <- sample(p_grid2, prob = posteriornorm2, size = 1000, replace = TRUE)
mean(samples2) #point estimate of the true proportion of water
quantile(samples2, probs = c(0.05, 0.95)) #95% CI
```

In short, given the _new_ prior and the observed data, our best estimate for the true proportion of water to land is ```r mean(samples2)``` (95%CI: ```r quantile(samples2, probs=(0.05))``` to ```r quantile(samples2, probs=(0.95))```). As we can see, this brings us close to the true proportion of $\sim 0.7$, as well as provides a narrower intervall.

The difference this different prior made can be easier to see visually:
```{r, echo=TRUE}
plot(density(samples1), xlim=c(0,1), ylim=c(0,6), col="blue", main="Density distributions for estimates of water proportion", sub="(1. in blue, 2. in red, and the correct proportion as a purple line)", xlab="Proportion of water", ylab="Density") #calling the first plot as well as setting label and x/y limits
lines(density(samples2), col="red") #calling the second plot
abline(v=0.7, col="purple") #calling the line for the true proportion
```

In summary: The new prior gives us a model with more AUC close to the real proportion, and a "peakyer" distribution. However as our (highly) limited sample-size was randomly skewed away from the real proportion, neither of these two models are very good at capturing the true proportion. 

***
### Task 3
This problem is more open-ended than the others. Feel free to collaborate on the solution. Suppose you want to estimate the Earth's proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of _p_ to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this? I won't require a precise answer. I'm honestly more interested in your approach.

### Task 3 - My solution
Assuming the toy globe have an accurate representation, and indendant tossing trials, which will eventually reach the true proportion, we can make a repeat loop that stops at the correct/needed amounts of throws to get a 99%CI of less than 0.05.
```{r, echo = TRUE}
kast <- 1
repeat {
kast <- kast+1
vann <- 7 * kast
total <- vann/0.7
p_grid3 <- seq(from = 0, to = 1, length = total)
prob_data3 <- dbinom(vann, size = total, prob= p_grid3)
posterior3 <- prob_data3
posteriornorm3 <- posterior3 / sum(posterior3)
set.seed(100)
samples3 <- sample(p_grid3, prob = posteriornorm3, size = total, replace = TRUE)
mean(samples3)
quantile(samples3, probs= c(0.005, 0.995))
sjekk <- quantile(samples3, probs = 0.995) - quantile(samples3, probs=0.005)
if (quantile(samples3, probs = 0.995) - quantile(samples3, probs=0.005) < 0.05){
  break
}
}
```

This "brute-force" method gives us the approximate needed numbers of throws:

```{r, echo=FALSE}
print(total)
```

And we can put this into our graph:

```{r, echo=FALSE}
plot(density(samples1), xlim=c(0,1), ylim=c(0,40), col="blue", main="Density distributions for estimates of water proportion", sub="(1. in blue, 2. in red, 3. in green and the correct proportion as a purple line)", xlab="Proportion of water", ylab="Density") #calling the first plot as well as setting label and x/y limits
lines(density(samples2), col="red") #calling the second plot
lines(density(samples3), col="green") #calling the third plot
abline(v=0.7, col="purple") #calling the line for the true proportion
```

For fun, we can add a final graph, showing the same model, but with $\sim$ half of the number of throws from above.

```{r, echo=FALSE}
total2 <- 1200
vann2 <- total2*0.7
p_grid4 <- seq(from = 0, to = 1, length = total2)
prob_data4 <- dbinom(vann2, size = total2, prob= p_grid4)
posterior4 <- prob_data4
posteriornorm4 <- posterior4 / sum(posterior4)
set.seed(100)
samples4 <- sample(p_grid4, prob = posteriornorm4, size = total2, replace = TRUE)
```

```{r, echo=FALSE}
plot(density(samples1), xlim=c(0,1), ylim=c(0,40), col="blue", main="Density distributions for estimates of water proportion", sub="(1. in blue, 2. in red, 3. in green, 4. in orange, and the correct proportion as a purple line)", xlab="Proportion of water", ylab="Density") #calling the first plot as well as setting label and x/y limits
lines(density(samples2), col="red") #calling the second plot
lines(density(samples3), col="green") #calling the third plot
lines(density(samples4), col="orange")
abline(v=0.7, col="purple") #calling the line for the true proportion
```

As we can see, the doubleing of throws does not do a whole lot for the model, this is because we enter an area of dimishing returnes, where each new case gets a smaller and smaller impact, due to the large number of previous trials (might not be the case in every setup).

***
## Week Two

***
### Task 1
The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% compatibility intervals for each of these individuals. That is, fill in the table below, using model-based predictions.

### Task 1 - My solution

Using the provided dataset, and code from the book (page 78->):
```{r, echo=FALSE, results='hide', message=FALSE}
library(rethinking)
```

```{r, echo=TRUE}
data(Howell1)
d <- Howell1
str(d)
dens(d)
```

As we can see, the data is somewhat messy, so we attempt to split the data into two groups, adults and children:

```{r}
dc <- d[d$age < 18,]
da <- d[d$age >= 18,]
dens(dc)
dens(da)
```

This make both sub-data set's distribution look a lot closer to normally distributed, as should be expected for weight and height. In our predictions, we use the adult subset, as this contains far more values for our weight from which height should be predicted.

Next we need some priors, I choose to use what I belive to be the average height now, minus 5cm, as this a old data-set, giving a mean of 167, with SD of roughly 30cm. We can plot this:

```{r}
curve(dnorm(x, 168, 30), from=100, to=250)
```

And a uniform SD prior:
```{r}
curve(dunif(x, 0, 50), from=-10, to=60)
```

Using quadratic approximation, assuming a linear relationsship: 
```{r}
xbar <- mean(da$weight)
m4.1 <- quap(
  alist(
    height ~ dnorm(mu,sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(168,30),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ),
data=da)
precis(m4.1)
```

Next we need to predict values for height, based on weight in our model:

```{r}
wgs <- data.frame(weight=seq(from=31, to=65, by=1))
estheight <- sim(m4.1, data=wgs)
Eh <- apply(estheight,2,mean)
Eh_ci <- apply(estheight,2,PI,prob=0.89)
dat <- cbind(wgs, Eh, Eh_ci[1,], Eh_ci[2,])
round(dat,1)
```

***
### Task 2
Model the relationship between height(cm) and thenatural logarithm of weight (log-kg): log(weight). Use the entire Howell1 data frame, all 544 rows, adults and non-adults. Use any model type from Chapter 4 that you think useful: an ordinary linear regression, a polynomial or a spline. Plot the posterior predictions against the raw data.

### Task 2 - My solution
Simply juse the code above (ordinary linear regression), slightly modified.
```{r}
d$logweight <- log(d$weight)
xbar <- mean(d$logweight)
m4.1 <- quap(
  alist(
    height ~ dnorm(mu,sigma),
    mu <- a + b * (logweight - xbar),
    a ~ dnorm(168,30),
    b ~ dlnorm(0,1),
    sigma ~ dunif(0,50)
  ),
data=d)
precis(m4.1)
```

And plotted against the raw data:
```{r}
plot( d$weight , d$height) 
x_seq <- log(1:60) #Huh?
mu <- sim(m4.1 , data=list(logweight=x_seq) )
mu_mean <- apply(mu,2,mean)
mu_ci <- apply(mu,2,PI,0.99) 
lines( exp(x_seq) , mu_mean, col='red' ) 
shade( mu_ci , exp(x_seq) )
```




